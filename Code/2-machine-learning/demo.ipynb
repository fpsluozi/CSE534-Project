{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the dataset files\n",
    "\n",
    "train20_nsl_kdd_dataset_path = \"NSL-KDD/KDDTrain+_20Percent.txt\"\n",
    "train_nsl_kdd_dataset_path = \"NSL-KDD/KDDTrain+.txt\"\n",
    "test_nsl_kdd_dataset_path = \"NSL-KDD/KDDTest+.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-defined features and attack categories from KDD\n",
    "\n",
    "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\", \"difficulty\"])\n",
    "\n",
    "col_names_true = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"])\n",
    "\n",
    "attack_dict = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "drop_cols = list(col_names[9:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "features_to_normalize = [\"duration\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "def preprocess_train(train_df):\n",
    "    ret_df = train_df.copy()\n",
    "    \n",
    "    # Categorize\n",
    "    for x in categorical_cols:\n",
    "        ret_df[x] = pd.Categorical(ret_df[x])\n",
    "        df_dummies = pd.get_dummies(ret_df[x], prefix = x)\n",
    "        ret_df = pd.concat([ret_df, df_dummies], axis=1)\n",
    "    ret_df = ret_df.drop(columns = categorical_cols)\n",
    "    \n",
    "    # Normalize non-categorical columns\n",
    "    for x in features_to_normalize:\n",
    "        if ret_df[x].max() > 0:\n",
    "            ret_df[x] = ret_df[x] / ret_df[x].max()\n",
    "    \n",
    "    return ret_df\n",
    "\n",
    "\n",
    "def preprocess_test(train_df, test_df):\n",
    "    ret_df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # Categorize\n",
    "    for x in categorical_cols:\n",
    "        ret_df[x] = pd.Categorical(ret_df[x])\n",
    "        df_dummies = pd.get_dummies(ret_df[x], prefix = x)\n",
    "        ret_df = pd.concat([ret_df, df_dummies], axis=1)\n",
    "    ret_df = ret_df.drop(columns = categorical_cols)\n",
    "    \n",
    "    ret_df = ret_df[len(train_df):]\n",
    "    \n",
    "    # Normalize non-categorical columns\n",
    "    for x in features_to_normalize:\n",
    "        if ret_df[x].max() > 0:\n",
    "            ret_df[x] = ret_df[x] / ret_df[x].max()\n",
    "    \n",
    "    return ret_df\n",
    "\n",
    "def get_xy(data_pcd):\n",
    "    X = data_pcd.drop(columns = ['labels', 'difficulty'])\n",
    "    y = data_pcd[\"labels\"].copy()\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == \"normal\":\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 1\n",
    "    y = y.astype('int')  \n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def get_xy_dos(data_pcd):\n",
    "    X = data_pcd.drop(columns = ['labels', 'difficulty'])\n",
    "    y = data_pcd[\"labels\"].copy()\n",
    "    for i in range(len(y)):\n",
    "        if attack_dict[y[i]] == \"DoS\":\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    y = y.astype('int')  \n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# loading and preprocessing the training set and the test set\n",
    "\n",
    "train_full = pd.read_csv(train_nsl_kdd_dataset_path, header=None)\n",
    "train_full.columns = col_names\n",
    "train_full = train_full.drop(columns=drop_cols)\n",
    "\n",
    "train_full_pcd = preprocess_train(train_full)\n",
    "train_X, train_y = get_xy(train_full_pcd)\n",
    "_, train_y_dos = get_xy_dos(train_full_pcd)\n",
    "\n",
    "test_full = pd.read_csv(test_nsl_kdd_dataset_path, header=None)\n",
    "test_full.columns = col_names\n",
    "test_full = test_full.drop(columns=drop_cols)\n",
    "\n",
    "test_full_pcd = preprocess_test(train_full, test_full) # Use this one for full one-hot encoding\n",
    "test_X, test_y = get_xy(test_full_pcd)\n",
    "_, test_y_dos = get_xy_dos(test_full_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "Train acc: 0.9931810784850722\n",
      "Test acc: 0.77861071682044\n"
     ]
    }
   ],
   "source": [
    "# Train a GradBoost model using attack-or-not labels\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "print(gbc.fit(train_X, train_y))\n",
    "print(\"Train acc:\", gbc.score(train_X, train_y))\n",
    "print(\"Test acc:\", gbc.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dos-1.txt True Acc\n",
      "Gradient Boosting 0.9999688686881265\n",
      "\n",
      "data/dos-2.txt True Acc\n",
      "Gradient Boosting 1.0\n",
      "\n",
      "data/dos-3.txt True Acc\n",
      "Gradient Boosting 1.0\n",
      "\n",
      "data/dos-4.txt True Acc\n",
      "Gradient Boosting 1.0\n",
      "\n",
      "data/dos-5.txt True Acc\n",
      "Gradient Boosting 0.9996877300202975\n",
      "\n",
      "data/dos-6.txt True Acc\n",
      "Gradient Boosting 1.0\n",
      "\n",
      "data/dos-7.txt True Acc\n",
      "Gradient Boosting 0.9999822964982473\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we verify the model's accuracy by feeding in live-captured data, which are preprocessed in the same way as the training/test sets.\n",
    "\n",
    "for i in range(1,8):\n",
    "    true_dos_path = \"data/dos-\" + str(i) + \".txt\"\n",
    "    print(true_dos_path, \"True Acc\")\n",
    "\n",
    "    true_full = pd.read_csv(true_dos_path, header=None)\n",
    "    true_full.columns = col_names_true\n",
    "    true_full['labels'] = 'pod'\n",
    "    true_full['difficulty'] = 1\n",
    "    true_full_pcd = preprocess_test(train_full, true_full) # Use this one for full one-hot encoding\n",
    "    true_X, true_y = get_xy(true_full_pcd)\n",
    "    _, true_y_dos = get_xy_dos(true_full_pcd)\n",
    "\n",
    "    print(\"Gradient Boosting\", gbc.score(true_X, true_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GradBoost model using DoS-or-not labels\n",
    "\n",
    "gbc2 = GradientBoostingClassifier()\n",
    "print(gbc2.fit(train_X, train_y_dos))\n",
    "print(\"Train acc:\", gbc2.score(train_X, train_y_dos))\n",
    "print(\"Test acc:\", gbc2.score(test_X, test_y_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,8):\n",
    "    true_dos_path = \"data/dos-\" + str(i) + \".txt\"\n",
    "    print(true_dos_path, \"True Accuracy\")\n",
    "\n",
    "    true_full = pd.read_csv(true_dos_path, header=None)\n",
    "    true_full.columns = col_names_true\n",
    "    true_full['labels'] = 'pod'\n",
    "    true_full['difficulty'] = 1\n",
    "    true_full_pcd = preprocess_test(train_full, true_full) # Use this one for full one-hot encoding\n",
    "    true_X, true_y = get_xy(true_full_pcd)\n",
    "    _, true_y_dos = get_xy_dos(true_full_pcd)\n",
    "\n",
    "    print(\"Gradient Boosting\", gbc2.score(true_X, true_y_dos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models in binary files\n",
    "from sklearn.externals import joblib\n",
    "models_path = \"./models/\"\n",
    "\n",
    "_ = joblib.dump(gbc, models_path + 'gbc_att_demo.pkl', compress=9)\n",
    "# _ = joblib.dump(gbc2, models_path + 'gbc_dos_demo.pkl', compress=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.77861071682044\n",
      "Actual outputs [1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "loaded_model = joblib.load(\"./models/gbc_att_demo.pkl\")\n",
    "print(\"Accuracy: \", loaded_model.score(test_X, test_y))\n",
    "print(\"Actual outputs\",loaded_model.predict(test_X[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the dataset files\n",
    "\n",
    "train20_nsl_kdd_dataset_path = \"NSL-KDD/KDDTrain+_20Percent.txt\"\n",
    "train_nsl_kdd_dataset_path = \"NSL-KDD/KDDTrain+.txt\"\n",
    "test_nsl_kdd_dataset_path = \"NSL-KDD/KDDTest+.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-defined features and attack categories from KDD\n",
    "\n",
    "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\", \"difficulty\"])\n",
    "\n",
    "col_names_true = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"])\n",
    "\n",
    "attack_dict = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "drop_cols = list(col_names[9:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "features_to_normalize = [\"duration\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "def preprocess_train(train_df):\n",
    "    ret_df = train_df.copy()\n",
    "    \n",
    "    # Categorize\n",
    "    for x in categorical_cols:\n",
    "        ret_df[x] = pd.Categorical(ret_df[x])\n",
    "        df_dummies = pd.get_dummies(ret_df[x], prefix = x)\n",
    "        ret_df = pd.concat([ret_df, df_dummies], axis=1)\n",
    "    ret_df = ret_df.drop(columns = categorical_cols)\n",
    "    \n",
    "    # Normalize non-categorical columns\n",
    "    for x in features_to_normalize:\n",
    "        if ret_df[x].max() > 0:\n",
    "            ret_df[x] = ret_df[x] / ret_df[x].max()\n",
    "    \n",
    "    return ret_df\n",
    "\n",
    "\n",
    "def preprocess_test(train_df, test_df):\n",
    "    ret_df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # Categorize\n",
    "    for x in categorical_cols:\n",
    "        ret_df[x] = pd.Categorical(ret_df[x])\n",
    "        df_dummies = pd.get_dummies(ret_df[x], prefix = x)\n",
    "        ret_df = pd.concat([ret_df, df_dummies], axis=1)\n",
    "    ret_df = ret_df.drop(columns = categorical_cols)\n",
    "    \n",
    "    ret_df = ret_df[len(train_df):]\n",
    "    \n",
    "    # Normalize non-categorical columns\n",
    "    for x in features_to_normalize:\n",
    "        if ret_df[x].max() > 0:\n",
    "            ret_df[x] = ret_df[x] / ret_df[x].max()\n",
    "    \n",
    "    return ret_df\n",
    "\n",
    "def get_xy(data_pcd):\n",
    "    X = data_pcd.drop(columns = ['labels', 'difficulty'])\n",
    "    y = data_pcd[\"labels\"].copy()\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == \"normal\":\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 1\n",
    "    y = y.astype('int')  \n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def get_xy_dos(data_pcd):\n",
    "    X = data_pcd.drop(columns = ['labels', 'difficulty'])\n",
    "    y = data_pcd[\"labels\"].copy()\n",
    "    for i in range(len(y)):\n",
    "        if attack_dict[y[i]] == \"DoS\":\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    y = y.astype('int')  \n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and preprocessing the training set and the test set\n",
    "\n",
    "train_full = pd.read_csv(train_nsl_kdd_dataset_path, header=None)\n",
    "train_full.columns = col_names\n",
    "train_full = train_full.drop(columns=drop_cols)\n",
    "\n",
    "train_full_pcd = preprocess_train(train_full)\n",
    "train_X, train_y = get_xy(train_full_pcd)\n",
    "_, train_y_dos = get_xy_dos(train_full_pcd)\n",
    "\n",
    "test_full = pd.read_csv(test_nsl_kdd_dataset_path, header=None)\n",
    "test_full.columns = col_names\n",
    "test_full = test_full.drop(columns=drop_cols)\n",
    "\n",
    "test_full_pcd = preprocess_test(train_full, test_full) # Use this one for full one-hot encoding\n",
    "test_X, test_y = get_xy(test_full_pcd)\n",
    "_, test_y_dos = get_xy_dos(test_full_pcd)\n",
    "\n",
    "train_X2 = train_X.drop(['count', 'srv_count'], 1)\n",
    "test_X2 = test_X.drop(['count', 'srv_count'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_true = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"])\n",
    "\n",
    "true_dos_path = \"dos-1.txt\"\n",
    "\n",
    "true_full = pd.read_csv(true_dos_path, header=None)\n",
    "true_full.columns = col_names_true\n",
    "true_full['labels'] = 'pod'\n",
    "true_full['difficulty'] = 1\n",
    "\n",
    "true_full_pcd = preprocess_test(train_full, true_full) # Use this one for full one-hot encoding\n",
    "true_X, true_y = get_xy(true_full_pcd)\n",
    "_, true_y_dos = get_xy_ddos(true_full_pcd)\n",
    "\n",
    "true_X2 = true_X.drop(['count', 'srv_count'], 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following 5 cells train the 5 selected ML models, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Train acc: 0.9720495661768792\n",
      "Test acc: 0.7663236337828248\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter = 1000, random_state=0, solver='lbfgs')\n",
    "print(clf.fit(train_X, train_y))\n",
    "print(\"Train acc:\", clf.score(train_X, train_y))\n",
    "print(\"Test acc:\", clf.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lawrencenewmbp/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Train acc: 0.9997221626856548\n",
      "Test acc: 0.7670777146912704\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "print(rfc.fit(train_X, train_y))\n",
    "print(\"Train acc:\", rfc.score(train_X, train_y))\n",
    "print(\"Test acc:\", rfc.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "Train acc: 0.9817897485969216\n",
      "Test acc: 0.7720014194464159\n"
     ]
    }
   ],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "print(abc.fit(train_X, train_y))\n",
    "print(\"Train acc:\", abc.score(train_X, train_y))\n",
    "print(\"Test acc:\", abc.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "Train acc: 0.9931810784850722\n",
      "Test acc: 0.778832505322924\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "print(gbc.fit(train_X, train_y))\n",
    "print(\"Train acc:\", gbc.score(train_X, train_y))\n",
    "print(\"Test acc:\", gbc.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "Train acc: 0.9959038841656546\n",
      "Test acc: 0.805624556422995\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "print(mlp.fit(train_X, train_y))\n",
    "print(\"Train acc:\", mlp.score(train_X, train_y))\n",
    "print(\"Test acc:\", mlp.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dos-1.txt True Acc\n",
      "Logistic Regression 0.18541809351846086\n",
      "Random Forest 0.0\n",
      "Adaptive Boosting 0.6513293070169976\n",
      "Gradient Boosting 0.9999688686881265\n",
      "MLP 3.1131311873482345e-05\n",
      "\n",
      "data/dos-2.txt True Acc\n",
      "Logistic Regression 0.2012372531849383\n",
      "Random Forest 1.412389480523149e-05\n",
      "Adaptive Boosting 0.9812717154882631\n",
      "Gradient Boosting 1.0\n",
      "MLP 1.412389480523149e-05\n",
      "\n",
      "data/dos-3.txt True Acc\n",
      "Logistic Regression 0.166725272517205\n",
      "Random Forest 5.023358617571708e-05\n",
      "Adaptive Boosting 0.9784497915306174\n",
      "Gradient Boosting 1.0\n",
      "MLP 2.511679308785854e-05\n",
      "\n",
      "data/dos-4.txt True Acc\n",
      "Logistic Regression 0.03539948149893943\n",
      "Random Forest 9.427292010370022e-05\n",
      "Adaptive Boosting 0.9875088380862598\n",
      "Gradient Boosting 1.0\n",
      "MLP 4.713646005185011e-05\n",
      "\n",
      "data/dos-5.txt True Acc\n",
      "Logistic Regression 0.1369303860995249\n",
      "Random Forest 0.0\n",
      "Adaptive Boosting 0.6907635001003725\n",
      "Gradient Boosting 0.9996877300202975\n",
      "MLP 0.0781790199183637\n",
      "\n",
      "data/dos-6.txt True Acc\n",
      "Logistic Regression 0.03026207103702035\n",
      "Random Forest 0.00015018397536982803\n",
      "Adaptive Boosting 0.23346098971239768\n",
      "Gradient Boosting 1.0\n",
      "MLP 2.5030662561638006e-05\n",
      "\n",
      "data/dos-7.txt True Acc\n",
      "Logistic Regression 0.0986616152674999\n",
      "Random Forest 0.0\n",
      "Adaptive Boosting 0.7565591473993556\n",
      "Gradient Boosting 0.9999822964982473\n",
      "MLP 1.7703501752646674e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and we run each trained model against our captured attack data from the SDN\n",
    "\n",
    "for i in range(1,8):\n",
    "    true_dos_path = \"data/dos-\" + str(i) + \".txt\"\n",
    "    print(true_dos_path, \"True Acc\")\n",
    "\n",
    "    true_full = pd.read_csv(true_dos_path, header=None)\n",
    "    true_full.columns = col_names_true\n",
    "    true_full['labels'] = 'pod'\n",
    "    true_full['difficulty'] = 1\n",
    "    true_full.head()\n",
    "    true_full_pcd = preprocess_test(train_full, true_full) # Use this one for full one-hot encoding\n",
    "    true_X, true_y = get_xy(true_full_pcd)\n",
    "    _, true_y_dos = get_xy_dos(true_full_pcd)\n",
    "\n",
    "\n",
    "    print(\"Logistic Regression\", clf.score(true_X, true_y))\n",
    "    print(\"Random Forest\", rfc.score(true_X, true_y))\n",
    "    print(\"Adaptive Boosting\", abc.score(true_X, true_y))\n",
    "    print(\"Gradient Boosting\", gbc.score(true_X, true_y))\n",
    "    print(\"MLP\", mlp.score(true_X, true_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following 5 cells train the same 5 selected ML models, but with the dos-or-not labels this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Train acc: 0.9881958832448223\n",
      "Test acc: 0.9233942512420156\n"
     ]
    }
   ],
   "source": [
    "clf2 = LogisticRegression(max_iter = 1000, random_state=0, solver='lbfgs')\n",
    "print(clf2.fit(train_X, train_y_dos))\n",
    "print(\"Train acc:\", clf2.score(train_X, train_y_dos))\n",
    "print(\"Test acc:\", clf2.score(test_X, test_y_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lawrencenewmbp/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Train acc: 0.9999523707461122\n",
      "Test acc: 0.8952271114265437\n"
     ]
    }
   ],
   "source": [
    "rfc2 = RandomForestClassifier()\n",
    "print(rfc2.fit(train_X, train_y_dos))\n",
    "print(\"Train acc:\", rfc2.score(train_X, train_y_dos))\n",
    "print(\"Test acc:\", rfc2.score(test_X, test_y_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "Train acc: 0.9991188588030768\n",
      "Test acc: 0.9068044712562101\n"
     ]
    }
   ],
   "source": [
    "abc2 = AdaBoostClassifier()\n",
    "print(abc2.fit(train_X, train_y_dos))\n",
    "print(\"Train acc:\", abc2.score(train_X, train_y_dos))\n",
    "print(\"Test acc:\", abc2.score(test_X, test_y_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "Train acc: 0.9997618537305613\n",
      "Test acc: 0.935193399574166\n"
     ]
    }
   ],
   "source": [
    "gbc2 = GradientBoostingClassifier()\n",
    "print(gbc2.fit(train_X, train_y_dos))\n",
    "print(\"Train acc:\", gbc2.score(train_X, train_y_dos))\n",
    "print(\"Test acc:\", gbc2.score(test_X, test_y_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "Train acc: 0.9985552459654052\n",
      "Test acc: 0.9238378282469837\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier()\n",
    "print(mlp2.fit(train_X, train_y_dos))\n",
    "print(\"Train acc:\", mlp2.score(train_X, train_y_dos))\n",
    "print(\"Test acc:\", mlp2.score(test_X, test_y_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dos-1.txt True Acc\n",
      "Logistic Regression 0.9997820808168856\n",
      "Random Forest 0.0\n",
      "Adaptive Boosting 0.0\n",
      "Gradient Boosting 0.999937737376253\n",
      "MLP 0.49698026274827223\n",
      "\n",
      "data/dos-2.txt True Acc\n",
      "Logistic Regression 0.8562187508827435\n",
      "Random Forest 0.6987938193836333\n",
      "Adaptive Boosting 0.0\n",
      "Gradient Boosting 0.9999858761051947\n",
      "MLP 0.0006496991610406486\n",
      "\n",
      "data/dos-3.txt True Acc\n",
      "Logistic Regression 0.8349826694127693\n",
      "Random Forest 0.7206510272768373\n",
      "Adaptive Boosting 0.0\n",
      "Gradient Boosting 1.0\n",
      "MLP 0.001105138895865776\n",
      "\n",
      "data/dos-4.txt True Acc\n",
      "Logistic Regression 0.9460287532406316\n",
      "Random Forest 0.8551967947207165\n",
      "Adaptive Boosting 0.0\n",
      "Gradient Boosting 0.9999528635399482\n",
      "MLP 0.00023568230025925054\n",
      "\n",
      "data/dos-5.txt True Acc\n",
      "Logistic Regression 0.8933151919345125\n",
      "Random Forest 4.460999710035019e-05\n",
      "Adaptive Boosting 0.0\n",
      "Gradient Boosting 0.9999776950014498\n",
      "MLP 0.5114759217540651\n",
      "\n",
      "data/dos-6.txt True Acc\n",
      "Logistic Regression 0.08778253360366449\n",
      "Random Forest 0.06257665640409502\n",
      "Adaptive Boosting 0.0\n",
      "Gradient Boosting 1.0\n",
      "MLP 0.006232634977847863\n",
      "\n",
      "data/dos-7.txt True Acc\n",
      "Logistic Regression 0.9999468894947421\n",
      "Random Forest 0.0\n",
      "Adaptive Boosting 0.0\n",
      "Gradient Boosting 0.9999645929964948\n",
      "MLP 0.24333463159012853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and we run each trained model against our captured attack data from the SDN\n",
    "\n",
    "for i in range(1,8):\n",
    "    true_dos_path = \"data/dos-\" + str(i) + \".txt\"\n",
    "    print(true_dos_path, \"True Acc\")\n",
    "\n",
    "    true_full = pd.read_csv(true_dos_path, header=None)\n",
    "    true_full.columns = col_names_true\n",
    "    true_full['labels'] = 'pod'\n",
    "    true_full['difficulty'] = 1\n",
    "    true_full.head()\n",
    "    true_full_pcd = preprocess_test(train_full, true_full) # Use this one for full one-hot encoding\n",
    "    true_X, true_y = get_xy(true_full_pcd)\n",
    "    _, true_y_dos = get_xy_dos(true_full_pcd)\n",
    "\n",
    "    print(\"Logistic Regression\", clf2.score(true_X, true_y_dos))\n",
    "    print(\"Random Forest\", rfc2.score(true_X, true_y_dos))\n",
    "    print(\"Adaptive Boosting\", abc2.score(true_X, true_y_dos))\n",
    "    print(\"Gradient Boosting\", gbc2.score(true_X, true_y_dos))\n",
    "    print(\"MLP\", mlp2.score(true_X, true_y_dos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the models into files for later use\n",
    "# where we can simply do\n",
    "# loaded_model = joblib.load(filename)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "models_path = \"./models/\"\n",
    "\n",
    "_ = joblib.dump(clf, models_path + 'lg_att.pkl', compress=9)\n",
    "_ = joblib.dump(rfc, models_path + 'rf_att.pkl', compress=9)\n",
    "_ = joblib.dump(abc, models_path + 'abc_att.pkl', compress=9)\n",
    "_ = joblib.dump(gbc, models_path + 'gbc_att.pkl', compress=9)\n",
    "_ = joblib.dump(mlp, models_path + 'mlp_att.pkl', compress=9)\n",
    "\n",
    "_ = joblib.dump(clf2, models_path + 'lr_dos.pkl', compress=9)\n",
    "_ = joblib.dump(rfc2, models_path + 'rf_dos.pkl', compress=9)\n",
    "_ = joblib.dump(abc2, models_path + 'abc_dos.pkl', compress=9)\n",
    "_ = joblib.dump(gbc2, models_path + 'gbc_dos.pkl', compress=9)\n",
    "_ = joblib.dump(mlp2, models_path + 'mlp_dos.pkl', compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
